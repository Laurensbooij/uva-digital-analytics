{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges for week 2\n",
    "\n",
    "Now that we've seen how to load and explore data in Pandas, it's time for you to apply this knowledge. This week has three challenges. Make sure to give it a try and complete all of them. \n",
    "\n",
    "**Some important notes for the challenges:**\n",
    "1. These challenges are a warming up, and help you get ready for class. Make sure to give them a try. If you get an error message, try to troubleshoot it (using Google often helps). If all else fails, go to the next challenge (but make sure to hand it in).\n",
    "2. While we of course like when you get all the answers right, the important thing is to exercise and apply the knowledge. So we will still accept challenges that may not be complete, as long as we see enough effort *for each challenge*. This means that if one of the challenges is not delivered (not started and no attempt shown), we unfortunately will not be able to provide a full grade for that week.\n",
    "3. Delivering the challenge on time on Canvas assignment is critical, as it helps also prepare for the DA live session. Check on Canvas how to hand it in.\n",
    "\n",
    "### Facing issues? \n",
    "\n",
    "We are constantly monitoring the issues on the GitHub general repository (https://github.com/uva-cw-digitalanalytics/2021s2/issues) to help you out. Don't hesitate to log an issue there, explaining well what the problem is, showing the code you are using, and the error message you may be receiving. \n",
    "\n",
    "**Important:** We are only monitoring the repository in weekdays, from 9.30 to 17.00. Issues logged after this time will most likely be answered the next day. This means you should now wait for our response before submitting a challenge :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting setup for the challenges\n",
    "\n",
    "We will use actual Twitter data for the challenges of this week. To do so, you need to download DMI-TCAT data that you may already be collecting for yourself, or from a colleague (if you haven't requested data collection yet).\n",
    "\n",
    "After exporting the data (see the export options from DMI-TCAT - *Export all tweets from the selection*), make sure you add the ```.csv``` file **in the same folder** where you are working with the assignment. \n",
    "\n",
    "**Note:** If you have a lot of tweets (over 200,000), it's probably better to select a smaller timeframe, so your dataset is not too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tcat_TheoAraujo-20200101-20200124------------fullExport--09043db5e1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22339"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Load the Twitter data in Pandas, and:\n",
    "1. Display the first few rows\n",
    "2. Check which columns the dataframe contains\n",
    "3. Check which columns contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Still with the Twitter data, please answer the following questions:\n",
    "1. How many tweets are there in the dataset?\n",
    "2. What is the average number of friends that the users have?\n",
    "3. What is the average number of followers that the users have?\n",
    "4. What is the average volume of retweets that the tweets have?\n",
    "5. What is the most popular language?\n",
    "\n",
    "**Make sure to run the code to get the information, and include your responses in markdown**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "We will run sentiment analysis on the tweets for next week. So this week, you need to **request** the sentiment analysis to us. For this challenge, you need to:\n",
    "1. Select only tweets that have language set to English (i.e., ```en```)\n",
    "2. Select only the columns ```id``` and ```text```\n",
    "\n",
    "After slicing the dataframe following the needs above you need to:\n",
    "1. Save the results to a pickle file containing your name, and the language of the tweets (in ISO two-letter codes). In my case, it would be:\n",
    "    * **TheoAraujo_EN.pkl**\n",
    "2. Upload the pickle file to SurfDrive (see the link in the homepage of the General Repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
